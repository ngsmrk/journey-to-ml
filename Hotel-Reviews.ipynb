{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution to the Hotel Reviews exercise\n",
    "\n",
    "All Rights Reserved Â© <a href=\"http://www.louisdorard.com\" style=\"color: #6D00FF;\">Louis Dorard</a>\n",
    "\n",
    "<img src=\"http://s3.louisdorard.com.s3.amazonaws.com/ML_icon.png\">\n",
    "\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "Load from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/data/hotel-reviews.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "pfr = ProfileReport(data)\n",
    "pfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.simple import SpaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk import download\n",
    "\n",
    "tknzr = SpaceTokenizer()\n",
    "stemmer = LancasterStemmer()\n",
    "download('stopwords')\n",
    "\n",
    "def filter_stopwords(text, stopword_list):\n",
    "    '''normalizes the words by turning them all lowercase and then filters out the stopwords'''\n",
    "    words=[w.lower() for w in text] #normalize the words in the text, making them all lowercase\n",
    "    #filtering stopwords\n",
    "    filtered_words = [] #declare an empty list to hold our filtered words\n",
    "    for word in words: #iterate over all words from the text\n",
    "        if word not in stopword_list and word.isalpha() and len(word) > 1: #only add words that are not in the French stopwords list, are alphabetic, and are more than 1 character\n",
    "            filtered_words.append(word) #add word to filter_words list if it meets the above conditions\n",
    "    # filtered_words.sort() #sort filtered_words list\n",
    "    return filtered_words\n",
    "\n",
    "def stem_words(words):\n",
    "    '''stems the word list using the English Stemmer'''\n",
    "    #stemming words\n",
    "    stemmed_words = [] #declare an empty list to hold our stemmed words\n",
    "    for word in words:\n",
    "        stemmed_word=stemmer.stem(word) #stem the word\n",
    "        stemmed_words.append(stemmed_word) #add it to our stemmed word list\n",
    "    # stemmed_words.sort() #sort the stemmed_words\n",
    "    return stemmed_words\n",
    "\n",
    "def concatenate(words):\n",
    "    s = \"\"\n",
    "    for word in words:\n",
    "        s = s + word + \" \"\n",
    "    return s\n",
    "\n",
    "def process_strings(row):\n",
    "    text = row['text']\n",
    "    filtered_words = filter_stopwords(tknzr.tokenize(text), stopwords.words('english'))\n",
    "    text_preprocessed = concatenate(stem_words(filtered_words))\n",
    "    return text_preprocessed\n",
    "\n",
    "%time data['text_preprocessed'] = data.apply(process_strings, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define X (inputs) and y (outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text_preprocessed'].tolist()\n",
    "y = data['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3 # ratio of data to have in test\n",
    "seed = 8 # to be used to initialize random number generator, for reproducibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "### Featurize training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many features we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apply the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Select best features\n",
    "\n",
    "We could make our models lighter and faster to train by selecting the `k` most \"promising\" features. One method for this is that of the $\\chi^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(chi2, k=50)\n",
    "selector.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apply feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect_select = selector.transform(X_train_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "See what we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect_select.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit(X_train_vect_select, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(selector.transform(vectorizer.transform(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"The Chicago Hotel was the best\"\n",
    "import textutils\n",
    "x_preprocessed = textutils.preprocess(x)\n",
    "print(x_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(selector.transform(vectorizer.transform([x_preprocessed])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model.predict_proba(selector.transform(vectorizer.transform([x_preprocessed])))\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability that it's fake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))\n",
    "pickle.dump(selector, open('selector.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hotel_reviews\n",
    "\n",
    "hotel_reviews.predict_fakeness(\"The Chicago Hotel was the best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model\n",
    "\n",
    "WARNING: The following is incorrect... `X_train_vect_select` was built after doing a \"fit\", and is passed as data on which to do grid_search. This includes cross validations, which perform train-test splits; such splits should be made before any \"fit\" is done.\n",
    "\n",
    "The right way is to define a pipeline first and then pass this pipeline to GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch RandomForest with AUC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy import arange\n",
    "params = arange(0.1, 1.0, 0.1)\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), {\"max_features\": params}, scoring=\"roc_auc\", cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_vect_select, y_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
    "autolaunch": true,
    "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

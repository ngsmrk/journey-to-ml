{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text pre-processing\n",
    "\n",
    "All Rights Reserved © <a href=\"http://www.louisdorard.com\" style=\"color: #6D00FF;\">Louis Dorard</a>\n",
    "\n",
    "<img src=\"http://s3.louisdorard.com.s3.amazonaws.com/ML_icon.png\">\n",
    "\n",
    "This notebook shows how to use [NLTK](http://www.nltk.org) to pre-process a textual feature in a Machine Learning dataset.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "- Pre-process\n",
    "- Split into training and test\n",
    "- Use CountVectorizer to fit training data\n",
    "- Use CountVectorizer to transform training data\n",
    "- Select best features from output of CountVectorizer\n",
    "- Transform training using best features\n",
    "- Create RandomForestClassifier \n",
    "- fit vectorized and selected version of x_train and y_train to RandomForestClassifier\n",
    "- predict using x_test\n",
    "- compute metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a function that filters out stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(text, stopword_list):\n",
    "    '''normalizes the words by turning them all lowercase and then filters out the stopwords'''\n",
    "    words=[w.lower() for w in text] #normalize the words in the text, making them all lowercase\n",
    "    #filtering stopwords\n",
    "    filtered_words = [] #declare an empty list to hold our filtered words\n",
    "    for word in words: #iterate over all words from the text\n",
    "        if word not in stopword_list and word.isalpha() and len(word) > 1: #only add words that are not in the French stopwords list, are alphabetic, and are more than 1 character\n",
    "            filtered_words.append(word) #add word to filter_words list if it meets the above conditions\n",
    "    # filtered_words.sort() #sort filtered_words list\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a function to stem words in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "def stem_words(words):\n",
    "    '''stems the word list using the English Stemmer'''\n",
    "    #stemming words\n",
    "    stemmed_words = [] #declare an empty list to hold our stemmed words\n",
    "    stemmer = LancasterStemmer()\n",
    "    for word in words:\n",
    "        stemmed_word=stemmer.stem(word) #stem the word\n",
    "        stemmed_words.append(stemmed_word) #add it to our stemmed word list\n",
    "    # stemmed_words.sort() #sort the stemmed_words\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define another function that concatenates all the words in a list and apply it to our list of stemmed words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(words):\n",
    "    s = \"\"\n",
    "    for word in words:\n",
    "        s = s + word + \" \"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application to Hotel Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "hotel_data = pd.read_csv('/data/hotel-reviews.csv', index_col=0)\n",
    "hotel_data.head()\n",
    "hotel_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = nltk.tokenize.simple.SpaceTokenizer()\n",
    "\n",
    "def process_hotel_strings(row):\n",
    "    text = row['text']\n",
    "    filtered_words = filter_stopwords(tknzr.tokenize(text), stopwords.words('english'))\n",
    "    text_preprocessed = concatenate(stem_words(filtered_words))\n",
    "    return text_preprocessed\n",
    "\n",
    "hotel_data['text-preprocessed'] = hotel_data.apply(process_hotel_strings, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    gucc sunglass stol bag fil report hotel sec an...\n",
       "1    gorg hotel outsid reach elev thing start look ...\n",
       "2    hotel impress upon ent staff howev felt room d...\n",
       "3    going internet retail last minut hotel left av...\n",
       "4    check rm next wok bed bug arm report man assig...\n",
       "Name: text-preprocessed, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_data['text-preprocessed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    My $200 Gucci sunglasses were stolen out of my...\n",
       "1    This was a gorgeous hotel from the outside and...\n",
       "2    The hotel is very impressive upon entering and...\n",
       "3    Going to the Internet Retailer 2010 at the las...\n",
       "4    I checked into this hotel, Rm 1760 on 11/13/20...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hotel_data['text'].tolist()\n",
    "y = hotel_data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.3 # ratio of data to have in test\n",
    "SEED = 8 # to be used to initialize random number generator, for reproducibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorise the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents='ascii', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8097"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select top 20 features and output accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max\n",
      "600\n",
      "0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "def predict(k):\n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    selector.fit(transformed_X_train, y_train)\n",
    "    selected_X_train = selector.transform(transformed_X_train)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(random_state=SEED)\n",
    "    model = classifier.fit(selected_X_train, y_train)\n",
    "    y_pred = model.predict(selector.transform(vectorizer.transform(X_test)))\n",
    "    import sklearn.metrics as metrics\n",
    "    return metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "max_accuracy = -1\n",
    "max_k = -1\n",
    "\n",
    "for k in range(50,len(vectorizer.get_feature_names()),50):\n",
    "    accuracy = predict(k)\n",
    "    if (accuracy > max_accuracy):\n",
    "        max_k = k\n",
    "        max_accuracy = accuracy\n",
    "        \n",
    "print(\"Max\")\n",
    "print(max_k)\n",
    "print(max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "livereveal": {
   "autolaunch": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
